\section{DE: Background}
\label{sec:DE}
\vspace{-0.4cm}
Storn and Price~\cite{Storn1997,storn95} proposed the Differential Evolution (DE) algorithm to solve global optimization problems over continuous search spaces. Due to its simple algorithmic framework and inexpensive computation in terms of CPU time but performing well in convergence, DE has emerged as one of the most competitive and versatile families of evolutionary algorithms. Moreover, DE has been widely applied and shown its strengths to solve a wide variety of very complex problems from diverse domains of science and technology~\cite{DAS2016,Price:2005}.  

DE is a population-based optimization method, having a very simple algorithmic structure, whose implementation requires only a few lines of code in any standard programming language. The optimal or near-optimal solution is obtained by an iterative process that is applied to a set of solutions (population) to achieve a new one. At each step of the process, known as iteration, new solutions arise as a result of perturbations to the current ones. 

The general DE structure shares similar features with other evolutionary algorithms (EAs), such as genetic algorithms (GAs)~\cite{holland75}. However, DE differs markedly from the well-known EAs because the first one reinforces the mutation as the principal perturbation operation. DE mutates the solutions with a scaled difference(s) of distinct members from the current population. Consequently, DE is different in handling distance and direction information to move the population at the current iteration toward the next one, in virtue of constructive cooperation between individuals.
The framework of DE is described in Algorithm~\ref{alg:algoritmoDE}. After the initialization step, an iterative process begins that includes the application of mutation, recombination, and selection operators. The iterations continue until a termination criterion is satisfied. The next sections explain each of the operations performed during the iterative process.

\subsection{Initialization}

The first step (Line 1 of Algorithm~\ref{alg:algoritmoDE}) consists in the initialization of the population $P^0$ of $N_P$ target vectors of \textit{D} real values ($x_i = (x_{i,1}, x_{i,2}, . . . , x_{i,D}) \in \mathbb{R}^{D}  (1 \leq i \leq N_P)$). Each vector forms a candidate solution to the multi-dimensional optimization problem. Usually, each $x_{i,j}$ is bounded to a value in the range $[lo_j, lu_j]$, where $lo_j$, $lu_j \in \mathbb{R}$ are the lower and upper bound, respectively. The $N_P$ target vectors are initialized randomly by applying Equation~\ref{eqDE:Init}.
\vspace{-0.2cm}
\begin{equation}\label{eqDE:Init}
x_{i,j} = lo_{j} + U(0, 1) \times (lu_j - lo_j)
\end{equation}
%\vspace{-0.1cm}
\noindent where $U(0, 1)$ is a random number with uniform distribution in the range $[0, 1]$. 

\begin{algorithm}[tb]
\scriptsize
    \caption{DE Algorithm } \label{alg:algoritmoDE}
    \begin{algorithmic} [1]
    \Require {$F, Cr, N_p$} 
    \Ensure {$x_{best}$} 
        \State initialize($P^0$,$N_p$) 
        \State $g \leftarrow 0$
        \While {not meet stop criterion}
            \For {each vector $x_{i}^g$ from $P^g$}
                \State $v_{i}^g \leftarrow $ mutate($x_{i}^g, P^g, F$) 
                \State $u_{i}^g \leftarrow $ recombinate($x_{i}^g, v_{i}^g, Cr$)
                \State $x_{i}^{g+1} \leftarrow $ select($x_{i}^g, u_{i}^g$)
                \State add($P^{g+1}, x_{i}^{g+1}$) 
            \EndFor
            \State $g \leftarrow g+1$ 
        \EndWhile
        \State$x_{best} \leftarrow $best\_solution($P^{g}$)
    \end{algorithmic}
\end{algorithm}


\subsection{Mutation}
The mutation operator (Line 5 of Algorithm~\ref{alg:algoritmoDE}) obtains a donor vector $v_i^g = (v_{i,1}, v_{i,2}, . . . , v_{i,D})$ for each target vector $x_i^g$ from the current population $P^g$ ($0 \leq g \leq max_{iter}$) following Equation~\ref{eqDE:mutation}. To obtain $v_i^g$, a base vector $x_{r0}^g$ and other two vectors $x_{r1}^g$ y $x_{r2}^g$ are randomly selected from $P^g$, with $r0, r1$ and $r2$ chosen from the set $\{1,2,...,N_P\}$ and all of them are mutually exclusive as well as different from $i$. 
\vspace{-0.1cm}
\begin{equation}\label{eqDE:mutation} 
v_i^g = x_{r0}^g + F \times (x_{r1}^g - x_{r2}^g)
\end{equation}
The $F \in [0 . . . 1)$ factor, known as scale factor, controls the rate at which the population evolves, in order to avoid their stagnation during the search process. The mutation operator is important to the DE's behaviour because it focuses the search on the most promising areas of the solution space. 

\subsection{Recombination}
The donor vector is modified by the recombination operator (Line 6), to increase the population diversity. This operator creates a trial vector $u_i^g$ through mixing components of the donor vector $v_i^g$ and the target vector $x_i^g$. The most frequently referred crossover operator is the binomial crossover, which is shown in Equation~\ref{eqDE:recombination}:
%\vspace{-0.4cm}
\begin{equation} \label{eqDE:recombination}
u_{i,j}^g = \left\lbrace
\begin{array}{ll}
v_{i,j}^g & \textup{si } r_j < Cr \vee j = j_r\\
x_{i,j}^g & \textup{otherwise} 
\end{array}
\right.
\end{equation}
%\vspace{-0.4cm}
\noindent where $r_j=U(0, 1)$ is a random value, $j_r$ is also a random value in the set \{1, 2, ...,$D$\}, and finally, $Cr$ is a parameter known as recombination probability, which controls the fraction of parameter values that are copied from the donor.

\subsection{Selection}
The last step in the DE's iterative process 
is the selection operation (Line 7). The trial vector $u_{i}^g$ competes against the target vector $x_{i}^g$ regarding their objective values (obtained applying the objective function to each vector). The best vector is selected to be part of the population $P^{g+1}$ of the next generation (Line 8) (see Equation~\ref{eqDE:selection}). This competition creates a new population with performance equal or superior to the current one. Consequently, DE is an elitist EA. 

\begin{equation} ~\label{eqDE:selection}
x_{i}^{g+1} = \left\lbrace
\begin{array}{ll}
u_{i}^g & \textup{if } f(u_{i}^g) \leq f(x_{i}^g)\\
x_{i}^g & \textup{otherwise} 
\end{array}
\right.
\end{equation}

\subsection{Stopping criterion}

The stopping criterion can be set to a preset maximum number of iterations ($max_{iter}$) or some other problem-dependent criterion. Whichever stop of the iterative process, the choice has a direct influence on the best solution $x_{best}$ obtained by the algorithm (Line 12). 

\subsection{DE Parameters}

DE performance mainly depends on three parameters: scaling factor of the difference vector ($F$), recombination control parameter ($Cr$), and population size ($N_P$). Some guidelines are available to choose the control parameters~\cite{Price:2005}. In this work, $N_P$ is chosen based on previous knowledge and keep it constant during all runs. The factor $F$ usually takes a value that ranges from 0.4 to 1.0~\cite{Gamperle02aparameter}. On the other hand, a good value for $Cr$ is 0.1. However, greater values can be used to speed up convergence. In consequence,  a good parameter setting can enhance the algorithm's ability to search for the global optimum or near-optimum with a high convergence rate. This assertion is the driving force behind one of the objectives of this work.